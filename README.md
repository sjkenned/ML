# Exploring Preprocessing for CNN's - subtracting local average color
# One preprocessing technique that can increase classification accuracy for some CNN's is to subtract each pixels neighbors' average color from that pixel. This has the effect of "standardizing" the color spectrum across all the images, which can reduce unwanting fitting to noise factors such as lighting. Ben Graham's winning Kaggle entry for diabetic retinopathy detection uses this method to its desired effect: https://kaggle2.blob.core.windows.net/forum-message-attachments/88655/2795/competitionreport.pdf. MNIST is an example of a dataset that is color-standardized "out of the box".
# While subtracting local average color may be the right choice for some classification tasks, in other cases it may decrease classification accuracy. If the overall color variation between images (not just relative color variation among the pixels in each image) is relevant to classification, the exclusion of that information from training and testing -which is the intended effect of local average color subtraction- could decrease classification accuracy. For example, if a CNN is distinguishing between apples and oranges, the colors of the fruits are obviously relevant to classification and subtracting local average color from all images would be a mistake. However, in the case of diabetic retinopathy detection, color variation between images results mostly from differences in lighting and is not relevant to classification. It is the relative color variations among the pixels of each image that encode the features of the eye (and thus are relevant to classification), so subtracting local average color improves accuracy.
# The code in this repository will explore the effect of subtracting local average color on CNN performance for different datasets.
